{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  import RegexpTokenizer() method from nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We use This when we are not very happy with the default stopwords like numbers are not present in stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'do', 'this']\n",
      "['I', \"can't\", 'do', 'this']\n",
      "['I', 'c', 'a', 'n', \"'\", 't', 'd', 'o', 't', 'h', 'i', 's']\n",
      "['I', 'c', 'a', 'n', 't', 'd', 'o', 't', 'h', 'i', 's']\n",
      "[' ', \"'\", ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sent1=\"I can't do this\"\n",
    "print(word_tokenize(sent1))\n",
    "print(regexp_tokenize(sent1,\"[\\w']+\"))\n",
    "print(regexp_tokenize(sent1,\"[\\w']\"))\n",
    "print(regexp_tokenize(sent1,\"[\\w]\"))\n",
    "print(regexp_tokenize(sent1,\"[\\W]\"))\n",
    "\n",
    "# word \"\\w\" followed by ' \n",
    "# plus (+) signify any number of these combination i.e now complete word is a single unit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \\w\\d\\s    ---> word digit whitespace\n",
    "###  \\W\\D\\S   ---> Not word digit whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'Python10', '30']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "\n",
    "# \\s is for white space characters\n",
    "\n",
    "# Create a reference variable for Class RegexpTokenizer \n",
    "tk = RegexpTokenizer('\\s+', gaps = True) \n",
    "gfg = \"I love Python10 30\"\n",
    "geek = tk.tokenize(gfg) \n",
    "\n",
    "print(geek) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love Python', ' ', ' ', ',']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "\n",
    "# \\d is for digits followed by any number of digits\n",
    "\n",
    "# Create a reference variable for Class RegexpTokenizer \n",
    "tk = RegexpTokenizer('\\d+', gaps = True) \n",
    "gfg = \"I love Python10 302012999 999,999\"\n",
    "geek = tk.tokenize(gfg) \n",
    "\n",
    "print(geek) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day.', 'The weather was cool and there were light showers.', 'I went to the market to buy some fruits.']\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'prateek@cb.com']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "\n",
    "document = \"\"\"It was a very pleasant day. The weather was cool and there were light showers. \n",
    "I went to the market to buy some fruits.\"\"\"\n",
    "\n",
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at prateek@cb.com\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sents = sent_tokenize(document)\n",
    "print(sents)\n",
    "print(len(sents))\n",
    "\n",
    "\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'prateek',\n",
       " '@',\n",
       " 'cb.com']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .split function and word_tokenize is not able to differ among 1,2,3\n",
    "### The differnce is word_tokenize function also break along Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapters', 'at', 'prateek@cb.com']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at prateek@cb.com\"\n",
    "\n",
    "# tokenizer is the object of class RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]+')\n",
    "useful_text = tokenizer.tokenize(sentence)\n",
    "print(useful_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# SnowballStemmer is a multilingual Stemmer i.e it supports other languages also\n",
    "# hence we need to specify the language unlike Porter Stemmer\n",
    "\n",
    "\n",
    "ss = SnowballStemmer('english')\n",
    "ss.stem('lovely')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x42 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 47 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# corpus is the document containg para graph about different genere\n",
    "\n",
    "corpus = [\n",
    "        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "        'The nobel laurate won the hearts of the people.',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story.'\n",
    "         ]\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "\n",
    "# fit will learn the words for the corpus i.e different words and transform will convert them into \n",
    "# different vectors\n",
    "\n",
    "\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorized_corpus is basically sparse matrix basically i.e \n",
    "###  (0, 6)\t1\n",
    "###  (0, 31)1\n",
    "\n",
    "\n",
    "### i.e in the sentence 0 , 6th word has a frequecy of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus = vectorized_corpus.toarray()\n",
    "vectorized_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 12, 'cricket': 6, 'team': 31, 'will': 37, 'wins': 39, 'world': 41, 'cup': 7, 'says': 27, 'capt': 4, 'virat': 35, 'kohli': 14, 'be': 3, 'held': 11, 'at': 1, 'sri': 29, 'lanka': 15, 'we': 36, 'win': 38, 'next': 19, 'lok': 17, 'sabha': 26, 'elections': 8, 'confident': 5, 'pm': 23, 'the': 32, 'nobel': 20, 'laurate': 16, 'won': 40, 'hearts': 10, 'of': 21, 'people': 22, 'movie': 18, 'raazi': 24, 'is': 13, 'an': 0, 'exciting': 9, 'spy': 28, 'thriller': 33, 'based': 2, 'upon': 34, 'real': 25, 'story': 30}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i.e which word is getting which vector index in BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It means there are 42 unique words in the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Mapping i.e generate the sentence out of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse Mapping!\n",
    "numbers = vectorized_corpus[2]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['hearts', 'laurate', 'nobel', 'of', 'people', 'the', 'won'],\n",
      "      dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "s = cv.inverse_transform(numbers)\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that these words are actually jumbled jump as it is a BOW Model where ordering is not important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "# Training the Word2Vec model\n",
    "# min_count 1 means donot consider the word with less than 1 count\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "\n",
    "words = model.wv.vocab\n",
    "\n",
    "# Finding Word Vectors\n",
    "vector = model.wv['war']\n",
    "print(vector.size)\n",
    "\n",
    "# Most similar words\n",
    "similar = model.wv.most_similar('vikram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('free', 0.19212336838245392),\n",
       " ('satish', 0.18654030561447144),\n",
       " ('economic', 0.18516503274440765),\n",
       " ('captured', 0.1770322024822235),\n",
       " ('self-reliant', 0.17524823546409607),\n",
       " ('came', 0.1739276647567749),\n",
       " ('poverty', 0.16935130953788757),\n",
       " ('brahm', 0.15850244462490082),\n",
       " ('enforce', 0.14135560393333435),\n",
       " ('dept', 0.13342753052711487)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most near words to the similar\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
